{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexLei2018/NLP/blob/main/Llama_3_download.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Llama 3 download"
      ],
      "metadata": {
        "id": "1trO6Vxto9pH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/meta-llama/llama3.git"
      ],
      "metadata": {
        "id": "dMEl6Xy_pI0B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adb734da-49b6-44ab-ccc3-9bd0983b1c00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama3'...\n",
            "remote: Enumerating objects: 347, done.\u001b[K\n",
            "remote: Counting objects: 100% (184/184), done.\u001b[K\n",
            "remote: Compressing objects: 100% (84/84), done.\u001b[K\n",
            "remote: Total 347 (delta 136), reused 132 (delta 100), pack-reused 163\u001b[K\n",
            "Receiving objects: 100% (347/347), 590.57 KiB | 5.57 MiB/s, done.\n",
            "Resolving deltas: 100% (195/195), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/llama3"
      ],
      "metadata": {
        "id": "K5vSFl2g0rDB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c649aae-2c3a-4b66-ea1c-74bdd308ef78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/llama3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -e ."
      ],
      "metadata": {
        "id": "S5tWIolh1XQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!bash download.sh"
      ],
      "metadata": {
        "id": "wn5-AbhWxgfo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4dfeb69-89ed-45d5-d6a9-00a180f41a55"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the URL from email: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#挂载google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "EreslfaLt9AK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#复制Meta-Llama-3-8B到google drive ，如下载70B,要替换source_dir1\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "source_dir1 = '/content/llama3/Meta-Llama-3-8B'\n",
        "target_dir = '/content/drive/My Drive/Llama3'\n",
        "\n",
        "# Ensure the target directory exists\n",
        "os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "# Copy the first directory\n",
        "dest_dir1 = os.path.join(target_dir, os.path.basename(source_dir1))\n",
        "if os.path.exists(source_dir1):\n",
        "    shutil.copytree(source_dir1, dest_dir1, dirs_exist_ok=True)\n",
        "else:\n",
        "    print(f\"Directory {source_dir1} does not exist!\")"
      ],
      "metadata": {
        "id": "e0nqoWLCv-lZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#复制Meta-Llama-3-8B-Instruct到google drive ，如下载70B,要替换source_dir2\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "source_dir2 = '/content/llama3/Meta-Llama-3-8B-Instruct'\n",
        "target_dir = '/content/drive/My Drive/Llama3'\n",
        "\n",
        "# Ensure the target directory exists\n",
        "os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "# Copy the second directory\n",
        "dest_dir2 = os.path.join(target_dir, os.path.basename(source_dir2))\n",
        "if os.path.exists(source_dir2):\n",
        "    shutil.copytree(source_dir2, dest_dir2, dirs_exist_ok=True)\n",
        "else:\n",
        "    print(f\"Directory {source_dir2} does not exist!\")\n",
        "\n",
        "print(\"Directories have been copied successfully.\")\n"
      ],
      "metadata": {
        "id": "UtmkctCoyFZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#免费colab账号由于内存限制无法运行\n",
        "!torchrun --nproc_per_node 1 example_text_completion.py --ckpt_dir /content/drive/MyDrive/Llama3/Meta-Llama-3-8B/ --tokenizer_path /content/drive/MyDrive/Llama3/Meta-Llama-3-8B/tokenizer.model --max_seq_len 512 --max_batch_size 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwRKytArCJVP",
        "outputId": "ed5d48d2-877e-4d55-882d-e371c77e24dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-04-20 06:13:17,009] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGINT death signal, shutting down workers\n",
            "[2024-04-20 06:13:17,009] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 11477 closing signal SIGINT\n",
            "> initializing model parallel with size 1\n",
            "> initializing ddp with size 1\n",
            "> initializing pipeline with size 1\n",
            "[2024-04-20 06:13:47,010] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 11477 via Signals.SIGINT, forcefully exiting via Signals.SIGKILL\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/torchrun\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 812, in main\n",
            "    run(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 803, in run\n",
            "    elastic_launch(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 135, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 259, in launch_agent\n",
            "    result = agent.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/metrics/api.py\", line 123, in wrapper\n",
            "    result = f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 727, in run\n",
            "    result = self._invoke_run(role)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 868, in _invoke_run\n",
            "    time.sleep(monitor_interval)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 62, in _terminate_process_handler\n",
            "    raise SignalException(f\"Process {os.getpid()} got signal: {sigval}\", sigval=sigval)\n",
            "  File \"/usr/lib/python3.10/enum.py\", line 781, in __format__\n",
            "    return cls.__format__(val, format_spec)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 62, in _terminate_process_handler\n",
            "    raise SignalException(f\"Process {os.getpid()} got signal: {sigval}\", sigval=sigval)\n",
            "torch.distributed.elastic.multiprocessing.api.SignalException: Process 11466 got signal: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JTQKuv6oIzSd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}